---
title: "Appendix B: System size expansion of the master equation for demographic noise"
author: 
  name: "Carl Boettiger"
  affiliation: a
address:
  - code: a
    address: "Dept of Environmental Science, Policy, and Management, University of California Berkeley, Berkeley CA 94720-3114, USA"
layout: 5p  
bibliography: ../paper/refs.bib
output: 
  rticles::elsevier_article:
    toc: true
  
preamble: "\\newcommand{\\ud}{\\mathrm{d}}"
---

\renewcommand{\thefigure}{B\arabic{figure}}


# Preliminary comments on notation

## Stochastic Differential Equations

@Ovaskainen2010 writes the canonical equation, Eq (1) in the main text, in the form:

$$ \frac{\textrm{d} n}{\textrm{d} t} = \underbrace{f(n)}_{\textrm{det skeleton}} + \underbrace{\sigma_d \xi_d(t) \sqrt{n}}_{\textrm{demographic noise}} + \underbrace{\sigma_e \xi_e(t) n}_{\textrm{environmental noise}}$$

rather than the more formal stochastic differential equation notation shown in the text.  Equations of this form are so-called "Langevin equation," popular in the physics literature [@vanKampen2007].  This notation looks like a more familiar differential equation, but the resemblence is misleading as the standard calculus does not apply to stochastic equations.  


A stochastic differntial equation (SDE) of the form

\begin{equation}
\textrm{d} X_t = \mu(X_t) \textrm{d}t + \sigma(X_t) \textrm{d}B_t \label{SDE}
\end{equation}

separates out the differential elements$\textrm{d}X_t$, $\textrm{d}t$ and $\textrm{d}B_t$ intentionally to remind the reader that the expression is short-hand not for a derivative expression, but for integrals:


\begin{equation}
X_{t+s} - X{t}  = \int_t^{t+s} \mu(X_{\tau}, \tau) \textrm{d}\tau + \int_t^{t+s} \sigma(X_{\tau}, \tau) \textrm{d}B\tau
\end{equation}


The first integral (over $\textrm{d}t$) is the familiar Reimann integral of the classic calculus, but the second is an Itô integral, which integrates over the stochastic process $B_t$ (Brownian motion, also termed a Weiner process). The Itô calculus follows different rules from the classic calculus, largely due to the fact that an Itô integral has non-zero quadratic variation. (Crudely put, $\textrm{d}x^2 \sim 0$, but $\textrm{d}B^2 \sim dx$).  The SDE notation also traditionally adopts capital letters such as $X_t$ to denote stochastic variables.  

The intepretation of Langevin equation is less precise, as it can be read to correspond to one of two possible stochastic integrals, and is often associated with a Stratonovich integral.  (Unlike the Itô integral, the Stratonovich integral has zero quadratic variation and so obeys the same chain rule as the classic calculus, but it is not a martingale, e.g. an unbiased random walk, which is a convenient for property for proving theorems).  Fortunately, the notion as a partial differential equation over probabilities does not have this ambiguity, which shows that @Ovaskainen2010 is referring to an Itô expression, just as I have written it more explicitly.  See @vanKampen2007 for an excellent discussion of the Itô-Stratonovich dillema and its intepretation with respect to intrinsic vs environmental noise factors. To avoid this ambiguity, mathematical literature will frequently write the Statonovich-type SDE explicitly as $\mathrm{d} X_t = f(X_t) \mathrm{d}t +  g(X_t) \circ  B_t$


### Partial Differential Equation formulation

An SDE can also be written in terms of an equivalent partial differential equation for the probability distribution of a given variable.  The SDE


\begin{equation}
\textrm{d} X_t = \mu(X_t) \textrm{d}t + \sigma(X_t) \textrm{d}B_t
\end{equation}

is equivalent to the PDE formulation [e.g. @vanKampen2007 or @Oksendal1985].


\begin{equation}
\frac{\partial}{\partial t} P(x,t) = - \frac{\partial}{\partial x} \mu(x) P(x,t) + \frac{1}{2} \frac{\partial^2}{\partial x^2} \sigma(x)^2 
\end{equation}



# Master equations and the step operator



\begin{figure}
\begin{center}
\includegraphics[width=.25\textwidth]{markov}
\caption{Birth and death as state-dependent transition rates in a Markov process.}\label{markov}
\end{center}
\end{figure}


Consider a population that is of size $n$ with probability $p_n$, with birth and death rates given by $b_n$ and $d_n$ respectively, diagrammed in Fig \ref{markov}  This representation is known as a single step Markov process.  The rate of transitions to the $n+1$ state $b_n$, the rate of transitions to $n-1$ is $d_n$.  Both of these are transitions away from the state $p_n$, hence the decrease the probability of $p_n$.  The probability $p_n$ increases by transitions into the state $n$, from either side: births enter from the state below at rate $b_{n-1}$ and deaths from the state above, $d_{n+1}$.  Hence the rate of change in $p_n$ is given by


\begin{align}
\dot p_n = b_{n-1} p_{n-1} + p_{n+1} d_{n+1} - [b_n + d_n]p_n
\end{align}



This probability balance is known for historical reasons as a master equation. This equation will form the center of our treatment.  Master equations of this form can be written more concisely by introducing the step operator, $\mathbb E^k$ such that $\mathbb E^k f_n = f_{n+k}$, giving us

\begin{align}
\frac{\ud p_n}{\ud t} = (\mathbb E^{-1} - 1) b_n p_n + (\mathbb E -1) d_n p_n \label{master1}
\end{align}

which for historical reasons [see @vanKampen2007] is known as a master equation. From this we can directly calculate the mean and variance for this process by multiplying by $n$ or $n^2$ and summing over all $n$. These calculations are greatly simplified by observing the following property of the step operator: for any pair of test functions $f_n$, $g_n$, 
\begin{align}
\sum_{n=0}^{N-1}  g_n \mathbb E f_n = \sum_{n=1}^N f_n \mathbb E^{-1} g_n \nonumber
\end{align}

For example, the mean is:

\begin{align}
\frac{\ud }{\ud t} \langle n \rangle &= \sum n (\mathbb E^{-1} - 1) b_n p_n + \sum n (\mathbb E -1) d_n p_n \nonumber \\ 
&= \sum  b_n p_n(\mathbb E - 1)n  + \sum d_n p_n(\mathbb E^{-1} -1) n \nonumber \\
&= - \langle d_n \rangle +  \langle b_n \rangle \label{mean}
\end{align}


and similarly the second moment is

\begin{align}
\frac{\ud }{\ud t} \langle n^2 \rangle &= 2 \langle n (b_n - d_n) \rangle + \langle b_n+d_n \rangle \label{var}
\end{align}

While all moments can be derived as above, it is often impossible to solve these equations for nonlinear models.  Moreover, we have yet to make any moment closure argument that solving for the first two moments in this manner will be sufficient.  We will illustrate how this is done in both the traditional diffusion approximation and then in the van Kampen system size expansion.   


## The system-size expansion vs the diffusion approximation

@Kramers1940 @Moyal1949, @Kurtz1978


@vanKampen1961 @vanKampen1976 @Kurtz1972 @Kurtz1978




A derivation of diffusion approximation in @Nisbet1982's classic textbook on fluctuating populations, or more recently in @Lande2003.  This diffusion approximation is also given for a more mathematically inclined audience in @Gardiner2009 and in @vanKampen2007.  The approach starts with the same master equation that defines the underlying dynamics of births and deaths (matching the process implemented in the exact Gillespie algorithm, @Gillespie1977, see Appendix A).  

The essence of the approach is to argue that in the limit of large populations, the discrete state for the number of indivduals in a population, $N$, can be replaced with a continuous variable, $n$, in which we can expand the master equation as a Taylor series and then truncate at second order.  In contrast to the system size expansion of van Kampen, this approach does not make system size explicit, but instead hinges on taking a simultaneous limit of both short time and small step size.  Consider small steps $x = \epsilon n$, and hence $p_n = \epsilon P_x$, where $\epsilon \ll 1$.  To keep the process from slowing down we consequently have to scale up the rates by $\epsilon$ as well:

\begin{align*}
	b_n - d_n = \frac{A_x}{\epsilon}
\end{align*}

Where $A$ is independent of $\epsilon$.  Similarly we have to scale the sum, which being proportional to the variance must be scaled by
\begin{align*}
b_n + d_n = \frac{B_x}{\epsilon^2}
\end{align*}

We Taylor expand the step operator $\mathbb E$ in the new variable:
\begin{align*}
\mathbb E^k &= 1+ \epsilon k\frac{\partial}{\partial x} + \frac{\epsilon^2 k^2}{2} \frac{\partial^2}{\partial x^2} + \ldots
\end{align*}

Then \eqref{master1} becomes:
\begin{align}
\frac{\partial P(x,t)}{\partial t} &= \left( \epsilon \frac{\partial }{\partial x} + \epsilon^2 \tfrac{1}{2}\frac{\partial^2 }{\partial x^2} + \ldots \right)\left(\frac{B_x}{2\epsilon^2}-\frac{A_x}{2\epsilon} \right)P \nonumber \\
+&\left(- \epsilon \frac{\partial }{\partial x} + \epsilon^2 \tfrac{1}{2}\frac{\partial^2 }{\partial x^2} - \ldots \right) \left( \frac{B_x}{2\epsilon^2} + \frac{A_x}{2\epsilon} \right)P 
\intertext{Collecting terms of common order $\epsilon$, }
& = - \frac{\partial A_x P}{\partial x} + \tfrac{1}{2} \frac{\partial^2 B_x P }{\partial x^2} + \mathcal O(\epsilon^2)\label{FP1}
\end{align}

For small $\epsilon$ we can ignore the terms of $\mathcal O(\epsilon^2)$, and we have recovered the diffusion approximation.  As noted above, we can write this PDE in the more compact notation of an SDE, substituting in our original terms for birth and death:

\begin{equation}
\ud X_t = \left[ b(x) - d(x) \right] \ud t + \sqrt{b(X_t) + d(X_t)} \ud W_t
\end{equation}






Unfortunately, we have not been very precise about what we mean by $\epsilon$ -- nature gives us no such obvious small parameter that we can use to "scale down" step sizes and "scale up" rates. The van Kampen expansion makes this notion more precise by introducing the concept of system size.  Instead of counts we consider a change of units into counts per system size, e.g. population density.  We can imagine drawing ever larger rings in our landscape of individuals, defining an ever bigger and bigger "system."  





## The System size expansion

 The method is based on derivation presented by the van Kampen [@vanKampen2007], though a formal proof can be found in earlier work of Thomas Kurtz [@Kurtz1970; @Kurtz1971; @Kurtz1972].  
 
 
 
 
 
 

The van Kampen system size expansion, shown for a general birth-death
process in Appendix B, derives an *ordinary* differential equation (ODE)
for the macroscopic process, coupled to a *linear* SDE which
governs the *deviations* ($\xi$)  from the macroscopic equation:


\begin{align}
\frac{\mathrm{d} x }{dt}  &= b(x) - d(x) + \mathcal{O}(N^{-1}) \\
\mathrm{d} \xi &=  \partial_{x}  \left( b(x)  - d(x) \right)  \xi \mathrm{d}t + \sqrt{b(x) + d(x)} \mathrm{d} B_t   + \mathcal{O}(N^{-\tfrac{1}{2}}
\end{align}

 
 
 
### Introduction: a change of variables

In this approximation, we expand the master equation \eqref{master1} in terms of a measure of the system size, $\Omega$.  The heart of this approximation is a change of variables, the rest is simply book-keeping.  We begin by explaining this change of variables, which replaces $n$ by some average value $\phi$ and some fluctuations $\xi$.   

Observe that \eqref{master1} is written in terms of discrete individuals, represented by the integer $n$.  Many population models permit real-valued variables for the population, usually interpreted as the density of individuals, for which fractional values have meaning.  If we go out to the field and mark off a very large area and count all the individuals within it, we can expect to get the population density, $\phi$.  Over some appropriate region, we expect the population density to be independent of our survey area.  Knowing the density, we can predict how many individuals we'd expect to find in any given area $\Omega$, simply $\phi \Omega$.  We also know that the larger the area, the more accurate our prediction.  We call $\phi$ a macroscopic variable -- it describes what we expect to see over an entire population (the macroscopic level) on average, rather than at the individual level.  It is an intensive (bulk) variable, because it does not depend on the area surveyed, while number $n$ will depend on the area $\Omega$ considered.  We expect $n$ to deviate around an average value of $\phi \Omega$ by some amount that depends on the system size.  For several reasons (such as the error term we found in the simple Poisson process), we will guess that the size of the fluctuations $\xi$ scale with system size as $\Omega^{1/2}$.  Mathematically, 

\begin{align}
n = \Omega \phi(t) + \Omega^{1/2} \xi \label{n}
\end{align}


We will change Eq \eqref{master1} into the variables $\phi$ and $\xi$.   We begin with the step operator, which can be approximated by a Taylor expansion.To formulate a Taylor expansion of the step operator (where $k$ can be a positive or negative integer), first consider the steop operator under the original discrete variable $n$:

\begin{align}
\intertext{Take as the definition}
\mathbb E^k f(n) &= f(n+k) \nonumber \\
\intertext{Then}
\mathbb E^k n &=  n+k \nonumber \\
\mathbb E^k n^2 &= \mathbb E^k n n = (n+k)(n+k) = n^2+2kn + k^2 \nonumber \\
\intertext{This suggests we can approximate the step operator by a Taylor series}
\mathbb E^k &= 1+ k\frac{\partial}{\partial n} + \frac{k^2}{2} \frac{\partial^2}{\partial n^2} + \ldots \label{En}
\end{align}
To change variables, recall the chain rule,
\begin{align}
&\frac{\partial}{\partial \xi} f(n(\xi)) =  \frac{\partial n}{\partial \xi} \frac{\partial}{\partial n} f(n(\xi)) \nonumber \\
\intertext{hence} 
&\frac{\partial}{\partial n} =  \left(\frac{\partial n}{\partial \xi}\right)^{-1} \frac{\partial}{\partial \xi} \nonumber 
\end{align}
Making this substitution to \eqref{En} we find:  


\begin{align}
& \mathbb E^k = 1+ \Omega^{-1/2} k\frac{\partial}{\partial \xi} + \Omega^{-1} \frac{k^2}{2} \frac{\partial^2}{\partial \xi^2} + \ldots \label{taylor}
\end{align}

Taking $P(n,t) = \Pi(\xi, t)$, we can rewrite the time derivative.  First, note that the derivative of the probability distribution in the master equation, $\tfrac{\partial}{\partial t} P(n,t)$ is taken with $n$ held constant,

\begin{align}
\frac{\ud n}{\ud t} &= \Omega \frac{\ud \phi(t)}{\ud t}  + \Omega^{1/2}\frac{\ud \xi}{\ud t} = 0, \nonumber \\
\intertext{therefore}
\frac{\ud \xi}{\ud t} &= -\Omega^{1/2}\frac{\ud \phi(t)}{\ud t}. \label{dxidt}
\end{align}

Also by the chain rule, we have
\begin{align*}
\frac{\partial}{\partial t} P(n,t) &= \frac{\partial \Pi}{\partial t}  + \frac{\partial \Pi}{\partial \xi}\frac{\ud \xi}{\ud t} 
\end{align*}
Substituting in \eqref{dxidt}, we find:  

\begin{align}
\frac{\partial}{\partial t} P(n,t) &= \frac{\partial \Pi}{\partial t}  - \Omega^{1/2}\frac{\ud \phi}{\ud t}\frac{\partial \Pi}{\partial \xi} \label{Pi}
\end{align}

To complete the transformation, we put \eqref{Pi} on the left side, replace all the $\mathbb E$'s in \eqref{master1} with \eqref{taylor}, and all the $n$'s appearing in the functions $b_n$ and $d_n$ with \eqref{n}:

\begin{align}
&\frac{\partial \Pi(\xi, t)}{\partial t}  - \Omega^{1/2}\frac{\ud \phi}{\ud t}\frac{\partial \Pi}{\partial \xi}  = \nonumber \\
&\quad \left( -\Omega^{-1/2} \frac{\partial}{\partial \xi} +  \frac{\Omega^{-1}}{2} \frac{\partial^2}{\partial \xi^2} + \ldots\right)b(\phi\Omega+\xi\Omega^{1/2})\Pi(\xi, t)\nonumber \\
+&\quad \left( \Omega^{-1/2} \frac{\partial}{\partial \xi} +  \frac{\Omega^{-1}}{2} \frac{\partial^2}{\partial \xi^2} + \ldots\right)d(\phi\Omega+\xi\Omega^{1/2})\Pi(\xi, t)
\end{align}


Collecting terms of order $\Omega^{1/2}$ on both sides, we have:

\begin{align}
\frac{\ud \phi(t)}{\ud t} = b(\phi)-d(\phi) = \alpha_1(\phi) \label{macroscopic}
\end{align}

Where we have the part of the birth and death functions that depend on the macroscopic variable $\phi$ alone.  This is known as the macroscopic equation, and corresponds to the density equations commonly written down.  Following @vanKampen2007, we will call this difference $\alpha_1(\phi)$ as a shorthand\footnote{This notation suggests it is the first jump moment, defined as first moment of the transition rate between states in a Markov process.  It so happens this term will give the macroscopic law for any Markov process, not only a birth-death process.  The second moment of the transition rates, $a_2$, will be for us simply the sum of the birth and death rates.  We use this notation as it generalizes to processes that have a different master equation from \eqref{master1}, to describe a transition of arbitrary step size: $\alpha_{i,j} = \int r^i \Phi_j(r) \ud r$.}   


Collecting terms of order $\Omega^0$ we recover the diffusion equation:

\begin{align}
\frac{\partial \Pi}{\partial t} = - \alpha'_{1,0}(\phi) \frac{\partial}{\partial \xi} \xi \Pi + \tfrac{1}{2}\alpha_{2,0}(\phi) \frac{\partial^2}{\partial \xi^2} \Pi \label{FP}
\end{align}

Where we define $\alpha_2(\phi) = b(\phi)+d(\phi)$ as an analogously.  From this it is a straight forward exercise to calculate the moments of the distribution (multiply by $\xi$, $\xi^2$ and integrate),

\begin{align}
\partial_t \langle \xi \rangle &= \alpha'_{1,0}(\phi) \langle \xi \rangle \\
\partial_t  \langle \xi^2 \rangle &= 2\alpha'_{1,0}(\phi) \langle \xi \rangle + \alpha_{2,0}(\phi) \label{fluc} 
\end{align}

The variance $\sigma_{\xi}^2 = \langle \xi^2 \rangle - \langle \xi \rangle^2$ obeys the same relation as the second moment. These equations must be solved using $\phi(t)$ from the macroscopic solution, solved with the appropriate initial condition $n_0$.  Then we can transform back to the original variables:

\begin{align}
\langle n \rangle &= \phi(t | n_0) \Omega + \Omega^{1/2} \langle \xi \rangle \label{n mean}\\
\sigma^2 &= \Omega \sigma_{\xi}^2 \label{n var}
\end{align}

It is possible to prove that any solution to \eqref{FP} must be Gaussian [@Kurtz1970; @Kurtz1971].  Consequently, knowing these two moments completely determines the distribution of $n$.  This is often assumed for demographic noise.  We have justified this common Gaussian noise assumption by showing it is simply a consequence of expanding the master equation to linear order, $\mathcal O(\Omega^{0})$.  From \eqref{n mean} we can also conclude that the macroscopic (average) variable obeys the deterministic law. 


## Fluctuation dissipation theorem

Eq \eqref{fluc} is particularly instructive.  Note that $\alpha_2(\phi)$ is always positive, and hence will try to increase the fluctuations $\langle \xi^2 \rangle$.  As this term increases, it increases the influence of its coefficient $\alpha_1'(\phi)$.  If this term is negative (as it must be near a stable equilibrium) then it serves to dissipate these fluctuations.  Note the dissipation comes from the macroscopic behavior alone, and does not depend on knowing $b$ and $d$ separately.  Since this dissipation is strongest for large fluctuations and small for very small fluctuations, $\xi^2$ will exponentially approach an equilibrium:

\begin{align}
\langle \xi^2 \rangle = \frac{-\alpha_2(\phi)}{2 \alpha_1'(\phi)} \label{fluctuation dissipation}
\end{align}

However, $\alpha_1'(\phi)$ need not be negative everywhere, but will be negative for any stable point, $\alpha_1'(\phi^s) < 0$.  Hence there will be a region around any stable point where this fluctuation-dissipation relationship given by \eqref{fluctuation dissipation} provides a good description of the fluctuations.  Consequently, for any birth-death process $b>d$ we can write down the equilibrium fluctuations as:

\begin{equation}
\sigma^2 = \frac{b+d}{2 [d' - b']} \label{fluc diss} 
\end{equation}


This expression has the wonderful properties of being both simple and general.  


## Example: The Levins patch model

The Levins meta-population model is given by

\begin{align}
\frac{\ud n}{\ud t} = c n \left(1 - \frac{n}{N}\right) - e n \label{levins}
\end{align}

where $n$ is the number of occupied patches, $N$ the total number of patches, $c$ is the colonization rate, and $e$ the extinction rate.  The colonization term provides our birth rate function and the extinction rate the death rate function.  The total number of patches $N$ is an obvious choice for the system size $\Omega$.  From this we can immediately apply the above theory.  The jump moments written in the macroscopic variable are:

\begin{align}
\alpha_1(\phi) &= c \phi(1- \phi) - e\phi \\
\alpha_2(\phi) &= c \phi(1-\phi) + e\phi
\end{align}

From which the macroscopic equation \eqref{macroscopic}, the fluctuations \eqref{fluc} can be solved.  According to \eqref{FP} the distribution is simply the Gaussian with the mean given by \eqref{n mean} and variance \eqref{n var}. The equilibrium of the macroscopic equation is:

\begin{align*}
\langle n\rangle_s = N\left(1-\frac{e}{c}\right)
\end{align*}
while the steady-state fluctuations are given by \eqref{fluctuation dissipation}, \eqref{n var}:
\begin{align*}
\sigma_n^2 = N\frac{e}{c}
\end{align*}


### Comparison to other models:

Note that the macroscopic equation for Levins' patch model has the same mathematical formulation as the familiar logistic equation, 

$$\frac{\ud n}{\ud t} = r n \left( 1 - \frac{n}{K} \right)$$
though the partition into birth and death rates is not explicit.  Different ways of dividing this equation between birth and death can therefore create different fluctuation patterns, even as the macroscopic average remains unchanged.  For instance, taking $b = rn$ and $d = rn^2/K$ and  we can calculate the fluctuations of the logistic equation around its equilibrium $n^* = K$ as:

\begin{align}
\sigma^2 = \frac{rn^* + rn^{*2}/K}{2(2rn^*/K - r)} = K \nonumber
\end{align}

Which agrees with calculations elsewhere [@Nisbet1982]. 


<!-- Include environmental noise example via stochastic K? -->





# An alternate derivation: the diffusion approximation

# System size expansion in higher-dimensional systems: environmental noise example

We introduce environmental noise into the master equation by allowing the transition rates to depend explicitly on the macroscopic variable $\psi$ as well as the state variable $\phi$.  It can be helpful to think of fluctuations in $\psi$ as also arising from some lower level transitions, but we need not model these explicitly.  Following the expansion as before (see @vanKampen2007 for details), the mean dynamics are given by the macroscopic equations:

\begin{align*}
\frac{\ud \phi}{\ud t} &= \alpha_{1,0}(\phi, \psi) \\
\frac{\ud \psi}{\ud t} &= \beta_{1,0}(\psi) 
\end{align*}

while the fluctuation dynamics are now given by three coupled differential equations:


\begin{align}
\frac{\ud \langle \xi^2 \rangle}{\ud t} &= 2 \frac{\partial \alpha_{1,0}}{\partial \phi} \langle \xi^2 \rangle + 2 \frac{\partial \alpha_{1,0} }{\partial \psi} \langle \xi \eta \rangle + \alpha_{2,0} \label{phivar} \\
\frac{\ud \langle \xi \eta \rangle}{\ud t} &= \left( \frac{\partial \alpha_{1,0}}{\partial \phi} + \frac{\partial \beta_{1,0} }{\partial \psi} \right)\langle \xi \eta \rangle + \frac{\partial \alpha_{1,0} }{\partial \psi} \langle \eta^2 \rangle \label{covar}\\
\frac{\ud \langle \eta^2 \rangle}{\ud t} &= 2 \frac{\partial \beta_{1,0}}{\partial \psi} \langle \eta^2 \rangle + \beta_{2,0} \label{psivar}
\end{align}

Without an explicit model for the environmental variation process, we can merely summarize its role in terms of the autocorrelation, $\tau_c := \tfrac{\partial \beta_{1,0}}{\partial \psi}$ and its overall variance at steady-state, $\langle \eta^2 \rangle = \tfrac{\beta_{2,0}}{-2 \partial_{\psi} \beta{1,0}}$, which we will refer to as the environmental variation $\sigma^2_e := \langle \eta^2 \rangle$.  A more detailed mechanistic description of how the environmental noise arises would permit a time-dependent solution, but the system size expansion tells us that these two summary statistics are all we need to express the steady-state solution above. 

Setting the remaining equations to zero to solve for the stationary state we can write down a general formula for the variation in $\phi$:

\begin{equation*}
\langle xi^2 \rangle = \frac{ \left( \partial_{\psi} \alpha_{1,0}(\phi,\psi) \right)^2}{\left( \partial_{\phi} \alpha_{1,0}(\phi,\psi) \right)^2 + \partial_{\phi} \alpha_{1,0} (x,y) \tau_c } \sigma_e^2 + \sigma^2_d
\end{equation*}

From which we recover the environmental variation equation presented in the main text.  


## Large deviations

Notably, both the SDE derived from the Kramer-Moyal diffusion approximation and the central limit theorem result arising from the system size expansion break down in the case of large deviations.  @Ovaskainen2010 provides an overview literature in that addresses this limit and discusses the use of the WKB^[Wentzel, Kramers & Brillouin. Note that the physicist Gregor Wentzel is not the same as the mathematician Alexander Wentzell of the Fredilin-Wentzell theorem for large deviations years later, though I believe the former's result can be established more formally by the theorem from the latter.] method to address this case. 



# References
